"""
Schema Generator

Compiles PlatformSpec entities into SQL migrations (PostgreSQL + SQLite fallback),
Pydantic models, and repository classes.
"""

from __future__ import annotations

from textwrap import dedent, indent

from forge.compiler.spec_schema import (
    Entity,
    EntityField,
    FieldType,
    PlatformSpec,
    RelationType,
)


# =============================================================================
# SQL Type Mapping
# =============================================================================

PG_TYPE_MAP: dict[FieldType, str] = {
    FieldType.STRING: "VARCHAR(255)",
    FieldType.TEXT: "TEXT",
    FieldType.INTEGER: "INTEGER",
    FieldType.FLOAT: "DOUBLE PRECISION",
    FieldType.DECIMAL: "NUMERIC(18,4)",
    FieldType.BOOLEAN: "BOOLEAN DEFAULT FALSE",
    FieldType.DATETIME: "TIMESTAMPTZ DEFAULT NOW()",
    FieldType.DATE: "DATE",
    FieldType.JSON: "JSONB",
    FieldType.UUID: "UUID DEFAULT gen_random_uuid()",
    FieldType.STRING_ARRAY: "TEXT[]",
    FieldType.INT_ARRAY: "INTEGER[]",
    FieldType.FLOAT_ARRAY: "DOUBLE PRECISION[]",
    FieldType.VECTOR: "vector({dim})",
    FieldType.BINARY: "BYTEA",
}

SQLITE_TYPE_MAP: dict[FieldType, str] = {
    FieldType.STRING: "TEXT",
    FieldType.TEXT: "TEXT",
    FieldType.INTEGER: "INTEGER",
    FieldType.FLOAT: "REAL",
    FieldType.DECIMAL: "REAL",
    FieldType.BOOLEAN: "INTEGER DEFAULT 0",
    FieldType.DATETIME: "TEXT DEFAULT (datetime('now'))",
    FieldType.DATE: "TEXT",
    FieldType.JSON: "TEXT",  # JSON stored as text
    FieldType.UUID: "TEXT",
    FieldType.STRING_ARRAY: "TEXT",  # JSON array
    FieldType.INT_ARRAY: "TEXT",
    FieldType.FLOAT_ARRAY: "TEXT",
    FieldType.VECTOR: "TEXT",  # JSON array fallback
    FieldType.BINARY: "BLOB",
}


def _pg_col(field: EntityField) -> str:
    """Generate a PostgreSQL column definition."""
    type_str = PG_TYPE_MAP[field.type]
    if field.type == FieldType.VECTOR:
        type_str = type_str.format(dim=field.vector_dimensions)

    parts = [f"    {field.name} {type_str}"]

    if field.unique:
        parts.append("UNIQUE")
    if field.required and field.default is None and field.type not in (
        FieldType.UUID, FieldType.DATETIME, FieldType.BOOLEAN
    ):
        parts.append("NOT NULL")
    if field.encrypted_at_rest:
        parts.append("-- ENCRYPTED (app-level AES-256-GCM)")

    return " ".join(parts)


def _sqlite_col(field: EntityField) -> str:
    """Generate a SQLite column definition."""
    type_str = SQLITE_TYPE_MAP[field.type]
    parts = [f"    {field.name} {type_str}"]

    if field.unique:
        parts.append("UNIQUE")
    if field.required and field.default is None and field.type not in (
        FieldType.UUID, FieldType.DATETIME, FieldType.BOOLEAN
    ):
        parts.append("NOT NULL")

    return " ".join(parts)


# =============================================================================
# Migration Generation
# =============================================================================

def generate_pg_migration(spec: PlatformSpec) -> str:
    """Generate a PostgreSQL migration file from platform spec."""
    lines = [
        f"-- Zuup Forge: Auto-generated migration for {spec.platform.display_name}",
        f"-- Platform: {spec.platform.name} v{spec.platform.version}",
        f"-- Generated by forge compiler",
        "",
        "-- Enable extensions",
        "CREATE EXTENSION IF NOT EXISTS pgcrypto;",
        "CREATE EXTENSION IF NOT EXISTS vector;",
        "",
    ]

    # Audit table (always created)
    if spec.features.get("audit_chain", True):
        lines.extend([
            f"-- Audit chain for {spec.platform.name}",
            f"CREATE TABLE IF NOT EXISTS {spec.platform.name}_audit (",
            "    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),",
            "    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),",
            "    platform VARCHAR(50) NOT NULL,",
            "    action VARCHAR(50) NOT NULL,",
            "    principal_id VARCHAR(255) NOT NULL,",
            "    entity_type VARCHAR(100) NOT NULL,",
            "    entity_id VARCHAR(255) NOT NULL,",
            "    payload_hash VARCHAR(64) NOT NULL,",
            "    prev_hash VARCHAR(64),",
            "    metadata JSONB DEFAULT '{}'",
            ");",
            f"CREATE INDEX idx_{spec.platform.name}_audit_entity ON {spec.platform.name}_audit(entity_type, entity_id);",
            f"CREATE INDEX idx_{spec.platform.name}_audit_time ON {spec.platform.name}_audit(timestamp DESC);",
            "",
        ])

    # Entity tables
    for entity in spec.entities:
        table_name = f"{spec.platform.name}_{entity.name.lower()}"
        cols = [
            "    id UUID PRIMARY KEY DEFAULT gen_random_uuid()",
            "    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()",
            "    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()",
        ]

        if entity.soft_delete:
            cols.append("    deleted_at TIMESTAMPTZ")

        if entity.versioned:
            cols.append("    version INTEGER NOT NULL DEFAULT 1")

        for field in entity.fields:
            cols.append(_pg_col(field))

        lines.append(f"-- Entity: {entity.name}")
        lines.append(f"-- {entity.description}" if entity.description else "")
        lines.append(f"CREATE TABLE IF NOT EXISTS {table_name} (")
        lines.append(",\n".join(cols))
        lines.append(");")
        lines.append("")

        # Indexes
        for field in entity.fields:
            if field.indexed:
                lines.append(
                    f"CREATE INDEX idx_{table_name}_{field.name} "
                    f"ON {table_name}({field.name});"
                )
            if field.searchable:
                lines.append(
                    f"CREATE INDEX idx_{table_name}_{field.name}_search "
                    f"ON {table_name} USING gin(to_tsvector('english', {field.name}));"
                )
            if field.type == FieldType.VECTOR:
                lines.append(
                    f"CREATE INDEX idx_{table_name}_{field.name}_vec "
                    f"ON {table_name} USING ivfflat({field.name} vector_cosine_ops) "
                    f"WITH (lists = 100);"
                )
        lines.append("")

    # Junction tables for many-to-many relations
    for entity in spec.entities:
        for rel in entity.relations:
            if rel.type == RelationType.MANY_TO_MANY and rel.via:
                junction = f"{spec.platform.name}_{rel.via.lower()}"
                source_table = f"{spec.platform.name}_{entity.name.lower()}"
                target_table = f"{spec.platform.name}_{rel.target.lower()}"
                lines.extend([
                    f"-- Junction: {entity.name} <-> {rel.target}",
                    f"CREATE TABLE IF NOT EXISTS {junction} (",
                    f"    {entity.name.lower()}_id UUID REFERENCES {source_table}(id),",
                    f"    {rel.target.lower()}_id UUID REFERENCES {target_table}(id),",
                    "    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),",
                    "    metadata JSONB DEFAULT '{}',",
                    f"    PRIMARY KEY ({entity.name.lower()}_id, {rel.target.lower()}_id)",
                    ");",
                    "",
                ])

    # Data retention policy comments
    if spec.compliance.audit_retention_days:
        lines.extend([
            f"-- RETENTION POLICY: Audit records retained for {spec.compliance.audit_retention_days} days",
            f"-- Implement via pg_cron or application-level cleanup",
            "",
        ])

    # Trigger for updated_at
    lines.extend([
        "-- Auto-update updated_at trigger",
        "CREATE OR REPLACE FUNCTION update_updated_at()",
        "RETURNS TRIGGER AS $$",
        "BEGIN",
        "    NEW.updated_at = NOW();",
        "    RETURN NEW;",
        "END;",
        "$$ LANGUAGE plpgsql;",
        "",
    ])

    for entity in spec.entities:
        table_name = f"{spec.platform.name}_{entity.name.lower()}"
        lines.extend([
            f"CREATE TRIGGER trg_{table_name}_updated_at",
            f"    BEFORE UPDATE ON {table_name}",
            f"    FOR EACH ROW EXECUTE FUNCTION update_updated_at();",
            "",
        ])

    return "\n".join(lines)


def generate_sqlite_migration(spec: PlatformSpec) -> str:
    """Generate a SQLite migration (for local dev / PodX air-gapped)."""
    lines = [
        f"-- Zuup Forge: SQLite migration for {spec.platform.display_name}",
        f"-- Platform: {spec.platform.name} v{spec.platform.version}",
        "",
        "PRAGMA journal_mode=WAL;",
        "PRAGMA foreign_keys=ON;",
        "",
    ]

    # Audit table
    if spec.features.get("audit_chain", True):
        lines.extend([
            f"CREATE TABLE IF NOT EXISTS {spec.platform.name}_audit (",
            "    id TEXT PRIMARY KEY,",
            "    timestamp TEXT NOT NULL DEFAULT (datetime('now')),",
            "    platform TEXT NOT NULL,",
            "    action TEXT NOT NULL,",
            "    principal_id TEXT NOT NULL,",
            "    entity_type TEXT NOT NULL,",
            "    entity_id TEXT NOT NULL,",
            "    payload_hash TEXT NOT NULL,",
            "    prev_hash TEXT,",
            "    metadata TEXT DEFAULT '{}'",
            ");",
            "",
        ])

    # Entity tables
    for entity in spec.entities:
        table_name = f"{spec.platform.name}_{entity.name.lower()}"
        cols = [
            "    id TEXT PRIMARY KEY",
            "    created_at TEXT NOT NULL DEFAULT (datetime('now'))",
            "    updated_at TEXT NOT NULL DEFAULT (datetime('now'))",
        ]

        if entity.soft_delete:
            cols.append("    deleted_at TEXT")

        if entity.versioned:
            cols.append("    version INTEGER NOT NULL DEFAULT 1")

        for field in entity.fields:
            cols.append(_sqlite_col(field))

        lines.append(f"CREATE TABLE IF NOT EXISTS {table_name} (")
        lines.append(",\n".join(cols))
        lines.append(");")
        lines.append("")

    return "\n".join(lines)


# =============================================================================
# Pydantic Model Generation
# =============================================================================

PYTHON_TYPE_MAP: dict[FieldType, str] = {
    FieldType.STRING: "str",
    FieldType.TEXT: "str",
    FieldType.INTEGER: "int",
    FieldType.FLOAT: "float",
    FieldType.DECIMAL: "Decimal",
    FieldType.BOOLEAN: "bool",
    FieldType.DATETIME: "datetime",
    FieldType.DATE: "date",
    FieldType.JSON: "dict[str, Any]",
    FieldType.UUID: "UUID",
    FieldType.STRING_ARRAY: "list[str]",
    FieldType.INT_ARRAY: "list[int]",
    FieldType.FLOAT_ARRAY: "list[float]",
    FieldType.VECTOR: "list[float]",
    FieldType.BINARY: "bytes",
}


def generate_pydantic_models(spec: PlatformSpec) -> str:
    """Generate Pydantic models from entity definitions."""
    lines = [
        '"""',
        f"Auto-generated Pydantic models for {spec.platform.display_name}",
        f"Platform: {spec.platform.name} v{spec.platform.version}",
        '"""',
        "",
        "from __future__ import annotations",
        "",
        "from datetime import date, datetime",
        "from decimal import Decimal",
        "from typing import Any",
        "from uuid import UUID",
        "",
        "from pydantic import BaseModel, Field",
        "",
        "",
    ]

    for entity in spec.entities:
        # Base model (for creation)
        lines.append(f"class {entity.name}Create(BaseModel):")
        if entity.description:
            lines.append(f'    """{entity.description}"""')
        lines.append("")

        for field in entity.fields:
            py_type = PYTHON_TYPE_MAP[field.type]
            if not field.required:
                py_type = f"{py_type} | None"

            field_args = []
            if field.description:
                field_args.append(f'description="{field.description}"')
            if field.min_length is not None:
                field_args.append(f"min_length={field.min_length}")
            if field.max_length is not None:
                field_args.append(f"max_length={field.max_length}")

            if field_args:
                default = "None" if not field.required else "..."
                lines.append(
                    f"    {field.name}: {py_type} = Field({default}, {', '.join(field_args)})"
                )
            elif not field.required:
                lines.append(f"    {field.name}: {py_type} = None")
            else:
                lines.append(f"    {field.name}: {py_type}")

        lines.extend(["", ""])

        # Full model (with DB fields)
        lines.append(f"class {entity.name}(BaseModel):")
        lines.append(f"    id: UUID")
        lines.append(f"    created_at: datetime")
        lines.append(f"    updated_at: datetime")
        if entity.soft_delete:
            lines.append(f"    deleted_at: datetime | None = None")
        if entity.versioned:
            lines.append(f"    version: int = 1")

        for field in entity.fields:
            py_type = PYTHON_TYPE_MAP[field.type]
            if not field.required:
                py_type = f"{py_type} | None"
                lines.append(f"    {field.name}: {py_type} = None")
            else:
                lines.append(f"    {field.name}: {py_type}")

        lines.extend(["", "    class Config:", "        from_attributes = True", "", ""])

        # Update model (all fields optional)
        lines.append(f"class {entity.name}Update(BaseModel):")
        for field in entity.fields:
            py_type = PYTHON_TYPE_MAP[field.type]
            lines.append(f"    {field.name}: {py_type} | None = None")
        lines.extend(["", ""])

    return "\n".join(lines)
